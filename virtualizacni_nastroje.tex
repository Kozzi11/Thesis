\section{Xen\textsuperscript{\textregistered}}
\xen je open source VMM (\textit{\textbf{V}irtual \textbf{M}achine \textbf{M}onitor}) též zvaný jako hypervizor, podporující jak 32 bitový tak i 64 bitový x86 procesory. Jedná se o jedno z z hodně rozšířených open source řešení, co se serverové virtualizace týče. Spousta profesionálních řešení s komerční podporou je vystavěna právě na základě Xen hypervizoru. Xen hypervizor běží přímo na fyzickém stroji (bare-metal) a umožňuje spouštět libovolný počet virtualizovaných strojů, které dosahují skoro stejného výkonu jako by se jednalo o nativní stroje \cite{hess:pract}. V následujícím textu se proto seznámíme s tímto virtualizačním řešením podrobněji.

\subsection{Historie}
Hypervizor \xen byl původně vyvinut jako součást univerzitního projektu XenoServers na Univerzitě v Cambridge. Kde hlavním cílem projektu bylo vyvinout veřejné rozhraní pro distribuovaný výpočet, jenž by bylo dostupné pro širokou veřejnost. Prvního zveřejnění se Xen hypervizor dočkal roku 2003. Jednalo se o dokument "`Xen and The Art of Virtualization"' zveřejněný na vědeckém zasedání o principech operačních systémů (\emph{Symposium on Operating Systems Principles}) \cite{xen:sym}. V tomto dokumentu je popsán hypervizor a jeho přístup řešení problému virtualizace na x86 architektuře.

Ve stejném roce vyšlo i první vydání Xen hypervizoru (verze 1.0), které bylo volně ke stažení. Od té doby se o Xen hypervizor začalo zajímat spousty firem, které použili tento hypervizor jako základ svých virtualizačních řešení. Xen byl později koupený firmou Citrix, která jej využívá pro své produkty XenServer. Samotný hypervizor však stále zůstal open source a jeho správu má na starost nadace XenSource založená jedním z původních vývojářů Ianem Prattem. Open source hypervizor se nachází na stránkách \url{http://www.xen.org/ }

\subsection{Architektura a způsob virtualizace}
\xen hypervizor podporuje primárně dva způsoby virtualizace stroje. První a původní je paravirtualizace, což je princip při kterém nedochází k emulaci jednotlivých komponent počítače. Namísto toho musí mít jádro hostovaného operačního systému (DomainU host) speciální ovladače pro univerzální rozhraní zařízení. Při přístupu na toto rozhraní se vlastně přistupuje nepřímo k reálnému HW. Zjednodušeně řečeno ovladač rozhraní přistupuje k ovladači (multiplexoru) v hypervizoru, který zajišťuje sdílení reálného HW mezi více hosty. Hypervizor pak následně přistupuje k reálnému HW pomocí ovladačů implementovaných v Domain0 hostu.

V předchozím odstavci jsme použil zatím nevysvětlené pojmy DomainU a Domain0 (zkráceně Dom0 a DomU). Samotný \xen hypervizor sám o sobě nemá žádné aplikační rozhraní, přes který by se dal ovládat a konfigurovat. K tomu je zapotřebí právě Dom0 host, což je v podstatě základní operační systém, který obsahuje ovladače k reálnému požitému hardware, nástroje pro práci s dalšími virtuálními hosty (DomU hosti) (vytváření, modifikování, spouštění\dots). Většinou je tímto základním Dom0 hostem některá z distribucí Linuxu. Dalšími podporovanými jsou některé operační systémy Unix a BSD distribuce jako NetBSD a OpenBSD.

Dom0 host s \xen hypervizor je v podstatě minimální možná použitelná virtualizační platforma, jelikož Dom0 host obsahuje plnohodnotný operační systém, na kterém mohou běžet libovolné služby a aplikace. Tato konfigurace však nemá příliš význam, proto se jako užitečné minimum udává konfigurace s alespoň s jedním DomU hostem. Jako DomU host se označuje jakýkoliv další virtuální host bežící na \xen hypervizoru, který není Dom0 hostem. Dom0 host běží vždy jen jeden, kdežto DomU hostů může běžet teoreticky neomezeně.

Jak už jsem naznačil DomU host může běžet buď v paravirtualizováném módu nebo v HVM (\emph{\textbf{H}ardware \textbf{V}irtual \textbf{M}achine}) módu. HVM mód se od paravirtualizovaného liší tím, že již není třeba modifikovat jádro operačního systému. A virtuální host se chová zcela stejně jako by se jednalo o reálný fyzický stroj. Toto je možné jen u novějších procesorů, které mají HW podporu virtualizace. Drobnou nevýhodou je nutnost emulace, některých HW komponent virtuálního stroje. Jelikož aby se virtuální stroj choval zcela jako fyzický a operační systém nepoznal rozdíl, tak je potřeba emulovat BIOS a základní HW. Existují však i způsoby jak může takto virtualizovaný host poznat, že je virtualizován a využívat zařízení podobně jak tomu je u paravirtualizace. Toho se zejména využívá u paměťových zařízení, síťového adaptéru a PCI. Jedním z pozitivních vlastností tohoto přístupu je nižší režie. Dalším rozdílem HVM oproti paravirtualizaci je schopnost spouštět hosty i s odlišnou architekturou. Například v paravirtualizovaném módu, pokud je Dom0 host 32bit tak i všichni DomU hosti musí bít 32bit, a nejen to dokonce záleží i na dalších parametrech. Pokud by Dom0 host měl jádro s podporou PAE (\emph{\textbf{P}hysical \textbf{A}ddress \textbf{E}xtension}), tak to samé musí platit pro DomU hosty. U HVM toto neplatí, zde platí obecně pravidlo, že DomU hosti mohou být technologicky odlišní, ale nesmí být na vyšší úrovni než Dom0 host. Pokud by tedy Dom0 měl 64bit architekturu, tak DomU může být 64bit, 32bit s PAE nebo 32bit host.

\subsubsection{Jaký mód tedy zvolit?}
Obecně se dá říct, že pokud máte stroj s procesorem podporující hardwarovou virtualizaci, tak je lepší použít novější HVM mód. V případě že máte procesor bez podpory virtualizace, tak je asi nejlepším řešením paravirtualizace. Pokud dokonce máte jen 32 bitový procesor, tak je určitě dobré použít jádra s podporou PAE. To vám umožní adresovat více než 4\,GB operační paměti.

\subsection{Výhody a nevýhody}
\subsubsection{Výhody}
\begin{itemize}
  \item možnost virtualizace i na strojích bez podpory hardwarové virtualizace
  \item problematické instrukce řešeny již během překladu a né v době běhu $\Rightarrow$ lepší výkon
  \item hodně rozšířené, velká podpora jak komunity tak existují i komerční řešení
  \item velké množství kvalitní literatury
  \item možnost plné virtualizace v případě podpory hardwarem
\end{itemize}

\subsubsection{Nevýhody}
\begin{itemize}
  \item v případě paravirtualizovaného hosta je potřeba mít modifikované jádro
  \item hypervizor není přímo součástí jádra operačního systému
\end{itemize}

\section{Kernel-based Virtual Machine}
KVM (\emph{\textbf{K}ernel-based \textbf{V}irtual \textbf{M}achine}) je dalším výkonné open source virtualizační řešení, které je primárně vhodné pro virtualizaci serverů. Jedná se relativně o mladé řešení, které se oproti \xen hypervizoru, ten se primárně soustředil na paravirtualizaci, dá spíše označit jako řešení podporující plnou virtualizaci. Ačkoliv si později ukážeme, že i v KVM se v určitých situacích využívá paravirtualizace. Důležitým rysem KVM je jeho závislost na HW podpoře virtualizace, bez které nelze KVM používat. Toto naštěstí v dnešní době není zas tak velký nedostatek a na většině dnešních strojů by KVM technologie měla být schopna fungovat. Původně byla podporována jen x86 architektura, ale během posledních let začala vznikat i podpora pro další architektury (powerpc, IA64, ARM \dots). Původním operačním systémem, na kterém bylo možno KVM provozovat byl operační systém s Linuxovým jádrem. Dnes však existují i porty pro další jádra operačních systému jako je FreeBSD a Illumos.

\subsection{Historie}
Jak už jsem se zmínil KVM je docela mladou technologií. Do jádra Linuxu se dostala teprve začátkem roku 2007 (jádro verze 2.6.20). KVM vyvinula docela neznámá firma Qumranet, a začlenění jejího kódu do jádra 2.6.20 proběhlo pro některé až velmi jednoduše. Důvodem pravděpodobně byl fakt, že si Linus Torvalds uvědomoval potřebu nativní podpory virtualizace v jádře a KVM se zdálo jako ideální řešení. V roce 2008 byla firma Qumranet odkoupena již mnohem známější firmou RedHat, Inc. Ta je nadále hlavním přispěvatelem co se kódu KVM týče a sama na KVM staví své nové komerční virtualizační řešení. Dříve RedHat stavěl právě na již zmiňovaném \xen hypervizoru. 

\subsection{Jak to funguje}
Ačkoliv KVM a \xen hypervizor dělají v podstatě to samé, tak každý z nich to dělá mírně odlišným způsobem. KVM sám o sobě totiž není hypervizor, jelikož neobsahuje žádné ovladače ani plánovač procesů atd. Jedná se v podstatě jen o modul jádra, který vytváří rozhraní pro vytváření virtuálních hostů uvnitř jádra, jako jaderné procesy. Dalo by se tedy říct, že roli hypervizoru u KVM hraje samotné jádro. Což má hned několik výhod, jako například, že není potřeba implementovat znovu plánovač procesů, ovladače a další součásti, které by jinak hypervizor musel mít. Využívají se totiž přímo algoritmy a ovladače v jádře, to má za následek přímočařejší přístup a následné lepší využití fyzických prostředků stroje.

KVM vyžaduje hardwarovou podporu virtualizaci, což v dnešní době obsahují jak procesory firmy Intel tak i procesory od AMD. Naneštěstí oba z těchto výrobců mají vlastní implementaci a sadu instrukcí, která je odlišná od konkurenčního řešení. Tento problém je v KVM vyřešen rozdělením do více jaderných modulů. Základní \texttt{kvm} modul obsahuje jen ty části, které se napříč oběma procesory neliší a jsou obecné. Na procesoru závislé úseky kódu jsou v jednotlivých vlastních modulech. U AMD procesorů je to modul s názevem \texttt{kvm-amd} a u procesorů Intel je to modul \texttt{kvm-intel}. Po načtení kvm a \texttt{kvm-intel} nebo \texttt{kvm-amd} modulů, se vytvoří zařízení \texttt{/dev/kvm}.
Pomocí tohoto zařízení skrze volání různých \texttt{ioctl()} příkazů  probíhá veškeré ovládání (vytváření, spouštění a mazaní hostů atd.) KVM virtualizace z uživatelského prostoru.

Důležitou součástí, bez které by nám KVM virtualizace nefungovala, je upravená verze emulátoru QEMU. Tento upravený emulátor poskytuje virtuálním hostům emulované komponenty jako je pevný disk, síťový adaptér, grafický akcelerátor atd. Na první pohled není rozdíl mezi použitím pouze QEMU, nebo KVM. Rozdíl je patrný až ve výkonu. Jelikož samotný QEMU pouze emuluje celý počítač a umožňuje na něm spouštět libovolný OS. Ale zde se jedná pouze o kompletní překlad všech instrukcí. To znamená že veškeré činnosti provedené v takto emulovaném hostu jsou překládány a emulovány a tudíž se nespouští přímo na fyzickém procesoru. To je samozřejmě velmi pomalé hlavně co se paměťových operací týče. Teprve až kombinace QEMU a KVM přináší požadovaný výkon a dá se zde mluvit přímo o virtualizaci. Prozatím, jak jsem již psal, je třeba modifikovaná verze QEMU, která umí spolupracovat s KVM. Do budoucna je však snaha začlenit veškeré potřebné změny přímo do zdrojového kódu oficiální verze QEMU. Což by mělo usnadnit a urychlit vývoj.

Z výše uvedeného textu vyplývá, že KVM virtualizace využívá plnou emulaci. To je částečně pravda jelikož tomu tak může být. Emulace sebou však nese režii navíc, což je nežádoucí. Proto bylo vymyšleno rozhraní Virtio, které má za úkol tento problém řešit. Jedná se vlastně o podobné řešení jaké jsem již zmiňoval u \xen hypervizoru. Místo toho, aby byli veškeré komponenty plně emulovány, tak pro ty nejnáročnější na výkon (disky, GPU, síťový adaptér\dots), byli napsány speciální ovladače využívající rozhraní virtio, což je vlastně paravirtualizované rozhraní. Tudíž se operace nemusí emulovat, ale jsou pomocí virtio ovladačů přenášeny přímo na reálný hardware hostitele.

\subsection{Výhody a nevýhody}
\subsubsection{Výhody}
\begin{itemize}
  \item přímo součástí jádra operačního systému $\Rightarrow$ netřeba doinstalovávat
  \item hypervizorem je vlastně samotné jádro, to přináší menší režii a použití ověřených algoritmů a ovladačů
  \item využití již existujících ověřených a funkčních nástrojů (QEMU)
  \item podpora paravirtualizace (Virtio)
  \item dělá jen jednu věc a pořádně
  \item rychlý vývoj
\end{itemize}
\subsubsection{Nevýhody}
\begin{itemize}
  \item poměrně nová technologie o které není pořádná literatura
  \item podpora pouze strojů s HW virtualizací (dnes už skoro irelevantní)
  %\item Virtio ovladače zatím dostupné pouze v Linuxu a Windows
\end{itemize}
\section{User Mode \linux}
\emph{\textbf{U}ser \textbf{M}ode \textbf{L}inux} (dále jen UML) je dalším zajímavým open source virtualizačním řešením. UML se od předchozích zmiňovaných technologií mírně liší. Dal by se totiž popsat spíše jako virtuální operační systém, než přímo jako virtuální stroj.

Jedná se totiž o port Linuxu na Linux, a to ve stejném smyslu jako porty Linuxu na jiné architektury procesorů (ARM, MIPS \dots). Pro lepší pochopení předchozí věty uvedu příklad. Pokud na svém stroji s x86\_64 procesorem spouštím Linux, tak jádro Linuxu volá instrukce mého procesoru (instrukční sadu x86\_64). V případě spouštění UML se jedná v podstatě o totéž. S tím rozdílem, že při spouštění UML se nevolají instrukce procesoru, ale jaderná volání. Pokud bychom tedy nahlédly do zdrojových kódů jádra Linuxu, tak v adresáři \texttt{arch}, kde jsou uloženy podporované architektury procesorů, najdeme kromě adresářů \texttt{x86\_64}, \texttt{arm}~\dots, také adresář s názvem \texttt{um}.

Každý UML virtuální stroj tedy běží jako samostatný proces. To znamená, že všechny virtuální stroje jsou izolované a pád, kteréhokoliv z nich neovlivní jak hostitele, tak ani další virtuální hosty. Další výhodou je velmi snadné a rychlé nastartování a vypnutí hosta. Díky tomu jak je UML navržen, je možné hosty během jejich běhu snadno rekonfigurovat a přidělovat jim systémové prostředky. Dokonce je možné jim přidělit více RAM nebo i procesorů, než má reálně hostitel. Toho se zejména využívá při testování softwaru, u kterého potřebujeme vyzkoušet funkčnost na hardwaru, který nemáme k dispozici.
\subsection{Historie}
Původním autorem UML je Jeff Dike, který začal na UML pracovat počátkem roku 1999. První veřejné oznámení se objevilo v mailové konferenci počátkem června. Následujících několik let bylo UML vyvíjeno primárně jen Jeffem a to mimo hlavní vývojovou větev.

To samozřejmě neslo několik nevýhod. Pokaždé když došlo, k některým změnám v kódu jádra. Musel Jeff tyto změny dohledávat v historii a upravovat svůj kód. Ačkoliv se z toho stala rutina, i tak si Jeff uvědomoval, že takto zabitý čas by se dal využít lépe. A to byl jeden z důvodů, proč se začal snažit protlačit svůj kód do hlavní větve. Druhým důvodem byla snaha více rozšířit UML mezi ostatní lidi.

Jeho snaha se nakonec vyplatila a 12. září 2002 byl UML začleněn do vývojové větve 2.5.34. Naneštěstí se později ukázalo, že není snadné přidávat nové funkce a opravy do hlavní větve jádra. Jelikož většina patchů, které Jeff poslal nebyla zařazena. A tak začal načas vyvíjet UML mimo hlavní větev. A to až do doby, kdy si jeden s vývojářů jádra těchto patchů všiml a nechal je zařadit do vývojové větve Andrew Mortona (Andrew Morton byl druhý ve vedení hned po Linusu Torvaldsovi). Zde UML zůstalo po určitou dobu a získalo si spoustu příznivců. Takže se nakonec opět dostalo do hlavní větve jádra (2.6.9) a od té doby je jeho součástí.

Prvních 5 let Jeff pracoval na UML jen ve svém volném čase a živil se jako IT konzultant. To se změnilo v roce 2004, kdy dostal pracovní nabídku od firem Intel a RedHat, Inc. Obě firmy mu nabídli možnost nadále pokračovat ve vývoji UML a Jeff nakonec začal pracovat pro Intel, kde se od roku 2004 nadále věnuje vývoji UML. Během následujících let se objevily informace, že Jeff pracuje na podpoře HW virtualizace, ale možná díky začlenění KVM do jádra k tomu nikdy nedošlo. 

\subsection{Výhody a nevýhody}
\subsubsection{Výhody}
\begin{itemize}
  \item každý virtuální stroj běží jako samostatný proces $\Rightarrow$ izolace
  \item možnost přidělovat více prostředků než je reálně dostupné
  \item přímo součástí jádra
  \item rychlé zapínání a vypínaní virtuálních hostů
\end{itemize}

\subsubsection{Nevýhody}
\begin{itemize}
  \item nejedná se o plnohodnotnou virtualizaci, což má určitá omezení
  \item nemožnost použití nemodifikovaných hostů
  \item podpora pouze Linux hostů
  \item né moc skvělý výkon, zatím chybí podpora HW virtualizace
\end{itemize}

\section{VirtualBox}
Oracle VM VirtualBox je multiplatformní virtualizační software, který je v současnosti vyvíjen společností Oracle Corporation. VirtualBox je samostatná aplikace, která ke svému běhu potřebuje spuštěný operační systém. Tím může být jeden ze čtveřice operačních systémů (Microsoft Windows, Mac OSX, Linux nebo Solaris). Krom zmíněných OS, zde existuje i snaha portovat VirtualBox na platformu FreeBSD. Během několika posledních let se z VirtualBoxu stal jeden z nejpopulárnějších virtualizačních softwarů. To dokazují i ankety z roku 2010, kde VirtualBox získal více než 50\,\% hlasů. VirtualBoxu se daří především na pracovních a domácích stanicích, a to hlavně díky svému jednoduchému grafickému rozhraní a dobré uživatelské dokumentaci.

VirtualBox se řadí mezi virtualizační software podporující plnou virtualizaci, a umožňuje spouštět nemodifikované hosty. Krom toho umožňuje své hosty spouštět ve dvou režimech. První režim nevyžaduje hardwarovou podporu virtualizace a problém privilegovaných instrukcí řeší pomocí nahrazování za běhu. K tomuto účelu slouží scanner instrukcí, který analyzuje volané instrukce a odchytává ty, které nemohli být spuštěny v daném ochranném režimu procesoru. Odchycené instrukce následně nahrazuje jejich bezpečnými variantami, které má předkompilované ve svém hypervizoru. Tento režim dosahuje díky dalším pokročilým technologiím celkem dobrého výkonu. Ale i přesto je zde režie okolo 20\,\%. Proto existuje i druhý režim využívající HW podporu virtualizace. V tomto režimu všechny hostované systémy zůstávají běžet v privilegovaném režimu a je jim udělen vlastní virtuální paměťový prostor. Díky tomu, že zde nedochází ke skenování a nahrazování privilegovaných operací je režie mnohem menší.

Stejně jako KVM podporuje Virtualbox v novějších verzích paravirtualizované Virtio rozhraní. To přináší další zvýšení výkonu v oblasti sítí a paměťových operací. Samozřejmě zde zůstává i podpora méně výkonných emulovaných zařízení, která se mohou hodit například z důvodů kompatibility se staršími hosty. Velkou výhodou je i podpora diskových formátů konkurenčních řešení, takže je možné pustit virtuální hosty, kteří byli původně instalovány například v programech od firmy VMWare (VMDK diskový formát) nebo Microsoft Virtual PC (VHD formát).

\subsection{Historie}

Vývoj VirtualBoxu započala německá firma InnoTek GmbH na začátku roku 2007, kdy vyšla první volně dostupná verze VirtualBoxu (1.3.2). Firma začala na produktu pracovat již dříve, ale tenkrát se jednalo o uzavřené řešení. K otevření došlo kvůli dodržení licenčních podmínek, jelikož produkt využívá část kódu z QEMU, který je vydáván po GNU GPL licencí.

Už tenkrát se jednalo o velmi zajímavý software, který zvládal většinu funkcí, které by jste od virtualizační-ho software čekali. O rok později proběhla akvizice firmy Innotek firmou Sun Microsystems, která byla zase následně koupena firmou Oracle Corporation.
Během prvního roku vyšlo hned několik zajímavých verzí, které postupně přidávali podporu pro nové hosty a technologie. Například ve verzi 1.4.0 vydané 06.06.2007 přibila podpora AMD64 a Mac OS X hostitelů. Podpora Windows 64bit hostitelů přibila v následující verzi (1.5.0). Krom podpory hostitelů přibili i funkce jako podpora USB 2.0 nebo podpora PulseAudio backendu. Další verze byli již pod záštitou společnosti Sun Microsystems. Zde asi nejzajímavější verze byla 2.0, kde přibyla podpora pro 64bitové hosty (jen na 64bitových hostitelích) a zvýšení výkonu u SATA řadičů v případě raw přístupu k zařízení (podpora technologie NCQ). Poslední verzí vydanou ještě pod záštitou Sunu byla verze 2.2.0. V této verzi již přibyla podpora pro 3D akceleraci a Host-only network mode.

Počínaje verzí 3.0 se již jednalo o produkt vyvíjený firmou Oracle Corporation, jelikož v pondělí 20. dubna 2009 byla společnost Sun Microsystems odkoupena firmou Oracle za 7,4 miliardy amerických dolarů. Zde stojí za zmínku verze 3.1.0, jenž přinesla podporu živé migrace hostů, 2D akceleraci pro Windows hosty a paravirtualizovaný síťový adaptér (virtio-net). V posledních verzí pak přibyla podpora pro klonování hostů, experimentální podpora Windows Aero a podporu více monitorů na Linuxových hostech.

\subsection{Výhody a nevýhody}
\subsubsection{Výhody}
\begin{itemize}
  \item podpora široké škály hostovaných operačních systémů 
  \item podpora paravirtualizace zařízení (virtio)
  \item HW podpora virtualizace
  \item umožňuje běh nemodifikovaných hostů
  \item velmi snadné ovládání a dostupnost kvalitní dokumentace
  \item podpora více operačních systému jako hostitelů
  \item velmi snadná instalace
\end{itemize}
\subsubsection{Nevýhody}
\begin{itemize}
  \item některé užitečné vlastnosti dostupné jen v non-free doplňcích
  \item podpora čistě x86 architektury
\end{itemize}
\section{\linux\ Containers}
LXC (\emph{\textbf{L}inu\textbf{x} \textbf{C}ontainers}) někdy též zvaný "`chroot na steroidech"' používá zcela odlišnou technologii virtualizace, než všechny dosud popsané virtualizační řešení. LXC se řadí mezi tzv. virtualizace na úrovni systému (\emph{operating system-level virtualization}). Tato technologie virtualizace zvolila zcela odlišní přístup. Nesnaží se totiž o vytvoření plnohodnotného virtuálního stroje, za pomocí emulace HW komponent a následného snižování režie pomocí paravirtualizace, ale soustředí se pouze na vytvoření izolovaného virtuálního prostředí, které umožňuje sdílet prostředky mezi až tisíci virtuálních prostředí.

Díky tomu, že všechny virtualizované systémy běží přímo na reálném HW a o sdílení prostředků se stará jedno jádro, je celková režie mnohonásobně menší než u řešení využívající plnou virtualizaci. Izolaci jednotlivých virtuálních systému řeší LXC pomocí funkcionalit jádra jako je cgroups a namespeces (jmenné prostory). LXC je docela mladou technologií, jelikož podpora jmenných prostorů a cgroups byla zařazena do jádra teprve nedávno.

LXC není jediným zástupcem této technologie na Linuxu. Existují zde i další velmi podobné technologie jako OpenVZ nebo Linux-VServer (o tom se zde zmíním podrobněji později). Ale LXC má jednu velkou výhodu oproti ostatním, a tím je jeho začlenění v hlavní větvi jádra Linuxu. To z něho dělá adepta na hlavní Linuxové řešení využívající tento druh virtualizace. Ještě stojí zmínit že virtualizace na úrovni systému, není dominantou jen Linuxu. Tento druh virtualizace je velmi kvalitně zpracován i na operačních systémech FreeBSD a Solaris (FreeBSD Jails a Solaris Containers).

Všechna tato řešení fungují na podobném principu. Základem této virtualizace je oddělení (izolace) procesů. Toho se dosahuje nastavením odlišného kořenového souborového systému (rootfs) pomocí systémového volání \texttt{chroot}, které bylo vyvinuto během vývoje sedmé edice Unixu, a to již v 80. letech 20. století. Dále dochází k izolaci na úrovni síťového subsystému, aby každý virtuální systém mohl mít svoji IP adresu. Toto se u LXC řeší právě pomocí jmenných prostorů.

\subsection{Vývoj}

Hlavním a původním vývojářem LXC je Daniel Lezcano, který je zaměstnán v IBM. První verze LXC (verze 0.1.0) byla vydána, alespoň dle historie v git repositáři, 6. srpna 2008. V té době bylo stabilní jádro 2.6.26, které sice už mělo podporu pro většinu potřebných jmenných prostorů, ale stále zde chyběla určitá podpora. Takže v té době bylo potřeba si jádro rozšířit o tuto podporu pomocí patchů. Nebo druhým a doporučovaným řešením bylo stáhnout si z git repositáře jádro, které již obsahovalo všechny potřebné patche.

Další verze jádra 2.6.27 již obsahovala vše nutné k provozu LXC. Ale stále zde pár věcí chybělo, takže i když s tímto jádrem bylo možné LXC provozovat, tak některé funkce nebyli dostupné. Kompletní podporu LXC mělo až jádro 2.6.29, které bylo vydáno 23. března 2009. Tedy až více než půl roku poté, co vyšla první verze LXC.

V té době bylo LXC ve verzi 0.6.0 a za ním následovalo několik vydání řady 0.6.x. tato vydání nepřínašela, krom drobností, žádné velké novinky. Spíše se jednalo o pročišťování kódu a odstraňování chyb. Určité zajímavé změny přinesla až verze 0.7.0. V té přibyli příkazy pro vypnutí a restartování kontejnerů (slovem kontejner se označují jednotlivá izolovaná virtuální prostředí). Druhou novinkou je podpora šablon pro příkaz lxc-create. To umožňuje snazší tvorbu kontejnerů za pomocí předpřipravených šablon. V době psaní této práce je aktuální stabilní verze 0.7.5, ta oproti 0.7.0 nepřinesla nic zvláštního až na opravy různých chyb. 

Nové vlastnosti se objeví až ve verzi 0.8.0. Která je momentálně ve stádia rc2 (release candidate 2). V této nové verzi přibude příkaz lxc-clone, umožňující klonování existujících kontejnerů. Dále byla přidána podpora pro tvorbu kontejnerů odlišných architektur (k tomu se částečně využívá QEMU). Navíc bylo LXC rozšířeno o podporu btrfs a lvm, a také přibylo několik předpřipravených šablon pro různé distribuce (Fedora, ArchLinux \dots).

\subsection{Výhody a nevýhody}

\subsubsection{Výhody}
\begin{itemize}
  \item technologie potřebné k využívání lxc jsou již obsaženy v jádře
  \item velmi malá režie
  \item není potřeba mít procesor s podporou hardwarové virtualizace
  \item šablony pro většiny známých distribucí
  \item velmi snadné používání a dostatek kvalitních návodů 
\end{itemize}

\subsubsection{Nevýhody}
\begin{itemize}
  \item nejedná se o kompletní virtualizaci $\Rightarrow$ menší míra izolace
  \item virtualizovaní hosti musí mít stejné jádro OS jako jejich hostitel
  \item hosti sdílí stejné jádro (nemožnost používat různá jádra pro potřebu kompatibility s legacy HW)
\end{itemize}
  
\section{\linux\ VServer\textsuperscript{\texttrademark}}

Linux-VServer je jeden z dalších zástupců využívající virtualizaci na úrovni systému. Stejně jako LXC vytváří jednotlivá izolovaná virtuální prostředí, kterým se zde však neříká kontejnery, ale jsou označovány jako \emph{security context}. Jednotlivým virtuálním systémům se pak říká virtuální privátní servery, zkráceně VPS (\emph{\textbf{V}irtual \textbf{P}rivate \textbf{S}ervers}).

Každý VPS je v podstatě samostatný Linuxový operační systém, na kterém mohou běžet libovolné nemodifikované aplikace a služby, tak jako by se jednalo o systém bežící mimo virtualizované prostředí. Všichni VPS mezi sebou sdílí stejné jádro a HW, což sebou nese určité výkonnostní výhody, ale i nevýhody co se izolace týče.

Tak jako u LXC se zde využívá některých technologií jádra, zejména capabilities. Ale krom technologií dostupných v jádře, Linux-VServer implementuje několik vlastních technologií např. podpora kontextů \cite{tomecek:lvs}. Díky tomu není možné používat přímo oficiální jádro Linuxu, ale je potřeba si jádro o podporu Linux-VServer rozšířit pomocí záplat (patch-ů). Což toto řešení dělá méně pohodlnou variantou než je konkurenční projekt LXC, co se instalace týče. Na druhou stranu pro většinu distribucí existují již předpřipraveně balíčky, tudíž není potřeba se o úpravu jádra příliš starat.

\subsection{Historie}
Počátky vývoje Linux-VServer sahají až do roku 2001. Tenkrát se projekt jmenoval jen VServer, a jeho původním vývojářem byl Jacques Gélinas. Ten vydal první verzi (0.1) dne 2. října 2001. Jacques pracoval na projektu VServer několik dalších let. Ale kolem roku 2003 se vývoj postupně zpomalil natolik, že se vývoje ujal další člověk. Tím byl Herbert Pötzl, který od té doby má na starost vydávání nových jaderných patchů, jejich oznamování na mailových konferencích a dává je k dispozici veřejnosti \cite{lvs:history}.

Krom jaderného kódu, prošly změnami i nástroje pro práci s Linux-VServerem. Ty byli Enrico Scholzem v roce 2003 přepsány do jazyka C a byly přejmenovány na util-vserver. Util-vserver se snaží být co nejvíce kompatibilní s původními nástroji napsanými pro VServer, ale mnohem více se drží vývojového modelu jaderných patchů.

V roce 2005 začal Benedikt Böhm pracovat na zcela nové verzi nástrojů pro ovládání Linux-VServeru, jenž měla být kompletně rozdílná a postavená zcela na odlišné architektuře. Tato nová implementace je známa pod jménem VServer Control Daemon, ačkoliv měla být dokončena do konce roku 2006. Do dnešního dne nevyšla žádná její verze.

\subsection{Výhody a nevýhody}

Jelikož se jedná o velmi podobné řešení jako je LXC, tak většina výhod a nevýhod zůstává stejných. Jediným hlavním rozdílem je to, že LXC má navíc tu výhodu, že všechny použité technologie jsou součásti jádra, kdežto Linux VServer potřebuje upravenou verzi jádra.

\section{OpenVZ}
Posledním nástrojem, o kterém se zde budu zmiňovat je OpenVZ. Tak jako v případě dvou předchozích řešení i zde se jedná o virtualizaci na úrovni systému. OpenVZ je dostupný zdarma pod GNU GPL licencí a je primárně vyvíjen firmou Parallels, Inc. Ta OpenVZ využívá jako základ pro svůj komerční produkt Parallels Virtuozzo Containers\footnote{\url{http://www.parallels.com/products/pvc/}} \cite{wiki:openvz}.

Jednotlivé virtuální servery jsou zde označovány názvem \emph{Virtual Private Server} (VPS), případně se také používá název \emph{Virtual Enviroment} (VE) \cite{tomecek:ovz}. Narozdíl od předchozích dvou řešení, OpenVZ nevyužívá k dosažení izolace prostředky dostupné v jádře a veškerou izolaci si řeší pomocí vlastních technologií. Stejně jak u Linux-VServer je zde potřeba aplikovat na jádra hostitelského systému záplaty, přidávající potřebné technologie.

\subsection{Historie}
V roce 2001 vydala firma SWsoft (dnes známá pod názvem Parallels, Inc) první verzi produktu Virtuozzo, určenou pro operační systém Linux. Později v roce 2005 vyšla i verze Virtuozzo pro operačním systém Microsoft Windows.
